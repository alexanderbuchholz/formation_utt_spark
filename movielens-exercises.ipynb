{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit for this notebook : Victor Hatinguais (former lecturer of this course)\n",
    "\n",
    "# Movie Recommendation with Spark MLlib\n",
    "\n",
    "In this notebook, we will use Spark MLlib to build a recommender system from MovieLens datasets.\n",
    "\n",
    "MovieLens is a project by GroupLens, a research laboratory at University of Minnesota, to provide a movies recommender application and use the collected data to improve algorithms. On https://movielens.org/, anyone can try the app for free and get movies recommandations. To help many people develop the best recommandation algorithms, MovieLens also released several datasets on http://grouplens.org/datasets/movielens/. We will use those datasets in this notebook.\n",
    "\n",
    "We will work with the two latest datasets available on MovieLens. The smallest one will help us build our application as fast as possible but you can use the biggest one whenever you want if you'd like to experience Spark power with a bigger dataset. Please keep in mind that we'll be using a free low capacity Spark cluster. Spark's scalability lets you run the same exact code on a much bigger cluster if you wish.\n",
    "\n",
    "The files to be uploaded from the MovieLens latest small dataset are:\n",
    "- movies.csv\n",
    "- ratings.csv\n",
    "Additional files may be uploaded depending on exercises, such as :\n",
    "- moviesBig.csv\n",
    "- ratingsBig.csv\n",
    "\n",
    "The small dataset is around 100k ratings. The biggest one is around 22M ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Bluemix, Jupyter & Markdown\n",
    "\n",
    "First, here are some links with useful information to help you answer the following exercices.\n",
    "\n",
    "This is a Databricks notebook environment where you can interactively develop Spark programs: https://docs.databricks.com/user-guide/notebooks/index.html\n",
    "\n",
    "Markdown is a simple markup language to structure text: https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\n",
    "\n",
    "Additional useful resources include:\n",
    "- Spark Python API documentation: http://spark.apache.org/docs/latest/api/python/\n",
    "    - SparkContext: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "    - RDD: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD\n",
    "    - sqlContext: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext\n",
    "    - DataFrame: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. MovieLens datasets: load & access\n",
    "\n",
    "Spark lets you explore data of any structure from a lot of different data sources and data formats.\n",
    "\n",
    "To load the data, upload them in the data section on the left pane.\n",
    "**Please only upload the smallest dataset.**\n",
    "\n",
    "You should get the paths to access the data from Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movies_path = \"/FileStore/tables/1a6s039m1508136002922/movies.csv\"\n",
    "ratings_path = \"/FileStore/tables/1a6s039m1508136002922/ratings.csv\"\n",
    "# moviesBig_path = \"/FileStore/tables/6lb85s4g1508136827033/movies.csv\"\n",
    "# ratingsBig_path = \"/FileStore/tables/6lb85s4g1508136827033/ratings.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, execute the following cell to be sure the data access is working fine.\n",
    "\n",
    "Notice that we are specifying an action - **first()** - to test the data access. Just using the **textFile()** method won't be sufficient because **textFile()** is a transformation so Spark only builds his DAG. Execution only takes place when an action is given, such as **first()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"First record in the movies.csv dataset:\", sc.textFile(movies_path).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be using data stored as text files in the Databricks File System. The structure is CSV (comma separated values) and is well-documented (see links below) but we'll be assuming that we don't even know the structure.\n",
    "\n",
    "- Small dataset documentation: http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html\n",
    "- Big dataset documentation: http://files.grouplens.org/datasets/movielens/ml-latest-README.html\n",
    "\n",
    "We will use two files from this MovieLens dataset: *ratings.csv* and *movies.csv*. All ratings are contained in the file *ratings.csv* and are in the following format:\n",
    "```\n",
    "userId,movieId,rating,timestamp\n",
    "```\n",
    "Movies information are in the file *movies.csv* and are in the following format:\n",
    "```\n",
    "movieId,title,genres\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are able to access the data, let's explore Spark functionalities.\n",
    "\n",
    "As you probably know any Spark session needs a SparkContext to submit jobs to an executors cluster. On this managed environment you were provided a free trial Spark cluster and a SparkContext is already available as **sc**.\n",
    "\n",
    "Refer to the Spark Python API documentation at http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext to learn what method you can call on SparkContext object.\n",
    "\n",
    "Below are some examples to get the Spark version running on your environment or the default parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this environment is running Spark 2.2.0 and the current default parallelism level is set to 8 which means at least eight partitions will be created for any new RDD. This is a good practice to configure the default parallelism to at least four times the number of executors to accomodate variance between the workloads.\n",
    "\n",
    "To create RDDs on the datasets, we use the method **.textFile()** which load the data from our Databricks File System. The second parameter let us specify the minimum number of partitions we want and the **.cache()** function has been added to be sure the dataset will be loaded and retained in memory as soon as we execute an action on it (remember Spark is lazy evaluation). Since we will be using those datasets a lot, it may be a good thing to load them in memory to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies = sc.textFile(movies_path, 10).cache()\n",
    "ratings = sc.textFile(ratings_path, 10).cache()\n",
    "# moviesBig = sc.textFile(moviesBig_path, 100).cache()\n",
    "# ratingsBig = sc.textFile(ratingsBig_path, 100).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was fast but remember that nothing happened yet. Spark just began to build an execution plan but is waiting you to provide an action before executing anything. The RDDs are however ready to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spark basics\n",
    "Let's discover Spark through simple commands first. Let say we know nothing about the dataset we just loaded. Those data could be unstructured, semi-structured or structured and contain any data format. Spark does not really care, the **textFile()** method let you load those files in RDDs and each line of those files is now an element of the RDDs.\n",
    "\n",
    "From this chapter, you will find some exercices. The places where you have to put code are marked with **#TODO: explanation**.\n",
    "\n",
    "First thing you want to know is what is in your dataset, how many elements do you have, what is the structure, the attributes types, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print movies.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *movies* RDD seems to be in CSV format and it is good to know there is an header.\n",
    "\n",
    "But to understand the data types, you probably want to get more lines. Use the Spark Python API documentation to find out how to retrieve 10 lines from both datasets *ratings* and *movies*.\n",
    "\n",
    "Notice that you probably don't want to retrieve **all lines**. In distributed computation, the dataset could be huge and it's probably a bad thing to retrieve all the data from executors on hundreds of machine to the driver on one single machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercice 1: get 10 elements from every dataset\n",
    "rats = ratings # TODO: get 10 elements\n",
    "print \"--------\\nRatings:\\n--------\"\n",
    "for r in rats:\n",
    "    print r\n",
    "movs = movies # TODO: get 10 elements\n",
    "print \"\\n--------\\nMovies:\\n--------\"\n",
    "for m in movs:\n",
    "    print m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that ratings elements are strings with comma separated values. The values are integers or floats.\n",
    "\n",
    "About movies, elements are strings with comma separated values. The values are strings, possibly with pipe separated values (for categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercice 2: print the number of elements in every dataset\n",
    "for rdd in movies, ratings:\n",
    "# for rdd in moviesBig, ratingsBig:\n",
    "    c = rdd #TODO: number of elements in rdd\n",
    "    print \"RDD '{}' has {} elements.\".format(rdd, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest dataset has 22M+ elements. If we experience computing delays, we may prefer work on the smaller dataset.\n",
    "\n",
    "While you are working in Spark with data from an input file, you usually start with this kind of RDDs of *lines* from your input file. But this input file probably has a structure or some specific elements that you want to extract from it in order to give your Spark RDD a structure. For example, this CSV file has four attributes: userId, movieId, rating and timestamp. Spark's RDD does not understand the data structure but you can give one to your data by splitting the lines on the comma separator.\n",
    "\n",
    "You'll learn later that another Spark data structure, named Spark's DataFrames, understand the data structure and is thus associated with a schema.\n",
    "\n",
    "But for now, prepare the RDDs by extracting the different fields and removing the header row and the timestamp field. You can also cast the fields in integer and float. Start with the small dataset, check the final RDD with **first()** or **take()** and once it's ok, we will duplicate your work on the bigger dataset. The **map()** method is the RDD's method that you are looking for if you wish to apply a function to any element of an RDD and get another RDD in return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercice 3: prepare the RDDs\n",
    "\n",
    "# Step 1: remove the header row\n",
    "movies2 = movies #TODO: Remove the header row from movies\n",
    "# Check that the new RDD has a row less\n",
    "print \"movies2 has \", movies2.count(), \" elements while movies has \", movies.count()\n",
    "ratings2 = ratings #TODO: Remove the header row from ratings\n",
    "# Check that the new RDD has a row less\n",
    "print \"ratings2 has \", ratings2.count(), \" elements while ratings has \", ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2: split the lines\n",
    "movies3 = movies2 #TODO: Split the lines to get an RDD of arrays of strings\n",
    "# Check an element of the new RDD\n",
    "print movies3.first()\n",
    "ratings3 = ratings2 #TODO: Split the lines to get an RDD of arrays of strings\n",
    "# Check an element of the new RDD\n",
    "print ratings3.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 3: take the fields you need (ie. remove the timestamp from ratings) and cast them as the appropriate data type\n",
    "movies4 = movies3 #TODO: Cast the fields to get an RDD of tuples (int, str, str)\n",
    "# Check an element of the new RDD\n",
    "print movies4.first()\n",
    "ratings4 = ratings3 #TODO: Cast the required fields to get an RDD of tuples (int, int, float)\n",
    "# Check an element of the new RDD\n",
    "print ratings4.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had a *ratings* RDD of strings representing lines in our input file.\n",
    "\n",
    "We now have a *ratings4* RDD of (integer, integer, float)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 4: apply all those operations at once to the dataset, you can make a line break in your code by putting an antislash (\\) before it\n",
    "\n",
    "movies4 = movies #TODO: apply all the above operations at once\n",
    "# Check an element of the new RDD\n",
    "print movies4.first()\n",
    "\n",
    "ratings4 = ratings #TODO: apply all the above operations at once\n",
    "# Check an element of the new RDD\n",
    "print ratings4.first()\n",
    "\n",
    "# If the biggest dataset is available in your environment, you may want to apply those operations to this dataset also. You can cache it for better performances.\n",
    "# moviesBig.cache()\n",
    "# ratingsBig.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with 22M+ records, the computation with Spark is super fast as long as the dataset can fit in memory and has been cached. This dataset is \"only\" 600MB. Imagine the possibilities with a cluster of ten 128GB RAM nodes for instance.\n",
    "\n",
    "With those RDDs, it will be easier to answer the two following exercices. In fact, it would be even easier if you were familiar with SQL (Standard Query Language) by the abstraction of DataFrames. Let's do it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercice 4: how many different users is there in the dataset and how many movies have been rated?\n",
    "print \"Number of different users:\", ratings4 #TODO\n",
    "print \"Number of different movies that have been rated:\", ratings4 #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercice 5: what are the maximum rating and the minimum rating that appear in the big dataset?\n",
    "print \"Rating max: \", ratings4 #TODO\n",
    "print \"Rating min: \", ratings4 #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercice 6: give the full distribution of the ratings, ie. number of occurences of each rating, you can help yourself with the WordCount example\n",
    "distribution = ratings4 #TODO\n",
    "# Check the results. You can collect() because you know that the resulting dataset is small\n",
    "for tuples in distribution.sortByKey().collect():\n",
    "    print tuples[0], \": \", tuples[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code, it is important to understand where the code executes. You should take advantage of your Spark's cluster power whenever possible and only manipulates small datasets on the driver single machine.\n",
    "\n",
    "Notice the distribution of the ratings is not uniform. We can represent it with a Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "import pandas\n",
    "distribution_pandas = pandas.DataFrame(distribution.sortByKey().collect(), columns=[\"rating\", \"freq\"])\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,12));\n",
    "distribution_pandas['freq'].plot(kind=\"bar\")\n",
    "ax.set_xticklabels(distribution_pandas['rating']);\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems users more often rate movies that they like than the one that they don't..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collaborative filtering\n",
    "Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix, in our case, the user-movie rating matrix. MLlib currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. In particular, we implement the alternating least squares (ALS) algorithm to learn these latent factors.\n",
    "\n",
    "What you want to estimate in our example is the rating a user would put to a movie he has not yet watched. If you are precise enough on that estimation, you just have to recommend to users the movies you have estimate he would have rated 4 or more if he had watched them.\n",
    "\n",
    "*ALS* is a supervised algorithm which means you need data that already contains the target you want to estimate, ie. the rating. You have this dataset, but it is usually a good thing to split this dataset in two. One, the training set, will be used to train the model. The other one, the test set will be used to evaluate your model performance by applying the model on the blind data (the data without the solution) and comparing the computed solution to the actual solution.\n",
    "\n",
    "So let's split the *ratings4* RDD between the training set and the test set with respectively about 80%/20% of the original dataset. The ratings should be split approximately randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercice 7: split the big dataset between a training set and a test set\n",
    "training = ratings4 #TODO\n",
    "test = ratings4 #TODO\n",
    "# Check the number of ratings in each RDD\n",
    "print \"Number of ratings in the training dataset: \", training.count()\n",
    "print \"Number of ratings in the test dataset: \", test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use MLlib’s *ALS* (Alternating Least Squares) to train a *MatrixFactorizationModel*, which takes a RDD[(user, product, rating)]. ALS has training parameters such as rank for matrix factors and regularization constants. To determine a good combination of the training parameters, we should randomly split the initial dataset between a training and a test datasets, train the model on the training set, evaluate it on the test set and iterate with different parameters. Let's just first try with some \"random\" parameters and compute the score with the RMSE (Root Mean Squared Error) for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the training parameters of ALS, the most important ones are rank, lambda (regularization constant), and number of iterations. The train method of ALS we are going to use is defined as the following:\n",
    "```python\n",
    "class ALS(object):\n",
    "\n",
    "    def train(cls, ratings, rank, iterations=5, lambda_=0.01, blocks=-1):\n",
    "        # ...\n",
    "        return MatrixFactorizationModel(sc, mod)\n",
    "```\n",
    "Ideally, we want to try a large number of combinations of parameters in order to find the best one but we won't for now. So let's train the model.\n",
    "\n",
    "You can additional documentation at: http://spark.apache.org/docs/1.4.1/mllib-collaborative-filtering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "model = ALS.train(training, 2, 5, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of algorithm may take some time on a huge dataset...\n",
    "\n",
    "Once the model is trained, we can apply it to the test set from which we remove the actual ratings (blind estimation). Then we join the predictions with the actual ratings and compute the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predictAll(test.map(lambda x: (x[0], x[1])))\n",
    "print \"An example of prediction: \", predictions.first()\n",
    "predictionsAndRatings = predictions \\\n",
    "    .map(lambda x: ((x[0], x[1]), x[2])) \\\n",
    "    .join(test.map(lambda x: ((x[0], x[1]), x[2])))\n",
    "print \"An example of predictions and ratings: \", predictionsAndRatings.first()\n",
    "\n",
    "from operator import add\n",
    "from math import sqrt\n",
    "n = test.count()\n",
    "testRMSE = sqrt(predictionsAndRatings.values().map(lambda x: (x[0] - x[1]) ** 2).reduce(add) / float(n))\n",
    "print \"RMSE = \", testRMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the **recommendProducts(user, nb)** method to recommand *nb* products (ie. movies) to a specific user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.recommendProducts(10,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4. To continue your journey with Spark\n",
    "### Comparing to a naive baseline\n",
    "Does ALS output a non-trivial model? We can compare the evaluation result with a naive baseline model that only outputs the average rating (or you may try one that outputs the average rating per movie). Computing the baseline’s RMSE is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meanRating = training.map(lambda x: x[2]).mean()\n",
    "baselineRMSE = sqrt(test.map(lambda x: (meanRating - x[2]) ** 2).reduce(add) / n)\n",
    "improvement = (baselineRMSE - testRMSE) / baselineRMSE * 100\n",
    "print \"The baseline RMSE is \", baselineRMSE, \", we improved it by \", improvement, \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic cross validation and parameters selection\n",
    "To improve the performance of our model, we could:\n",
    "- correct the ratings above 5 or under 0.5\n",
    "- round to the nearest .5 or not\n",
    "- select better parameters through automatic cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "It would be a good thing to build a confusion matrix to understand where our model is good and where it is bad, ie. a matrix 10x10 where the lines are the ratings, the columns are the predictions and each cell gives the number of predictions we made in each class (ie. 0.5, 1.0, 1.5, etc.) for each real rating. Thus, the matrix diagonal should ideally contain all the values. This matrix could also be plot as a heatmap through Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames (SparkSQL)\n",
    "\n",
    "Try to do exercices 4, 5, 6 through SQL thanks to Spark DataFrames. You'll have to define a schema and to use the **toDF()** method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "name": "exercices",
  "notebookId": 815315517711799
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
